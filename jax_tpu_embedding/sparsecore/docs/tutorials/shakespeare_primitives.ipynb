{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_wsiwhfwwmW"
   },
   "source": [
    "# The JAX SC Shakespeare Example - Primitives\n",
    "\n",
    "_An example Shakespeare model that uses the low-level primitives of the SparseCore embedding API_\n",
    "\n",
    "Using some text from the Shakespeare corpus, we're going to construct a model that predicts the next word for a given sequence of words. This tutorial is based on the working example in [.../examples/shakespeare/jax_sc_shakespeare_jit.py](https://github.com/jax-ml/jax-tpu-embedding/blob/main/jax_tpu_embedding/sparsecore/examples/shakespeare/jax_sc_shakespeare_jit.py) using the low-level primitves for the embedding lookup and gradient updates.\n",
    "\n",
    "In this example we'll walk through the construction and configuration of the model paying particular attention to the embedding aspects.\n",
    "\n",
    "This example domonstrates the following features of the JAX SC API:\n",
    "- Input preprocessing\n",
    "- TableSpec and FeatureSpec creation\n",
    "- `jax.jit` + `shard_map` execution\n",
    "- Direct execution of embedding lookup and gradient update for embeddings\n",
    "- FDO (feedback directed optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52eYRZZCaIep"
   },
   "source": [
    "# Front matter\n",
    "\n",
    "We start off with some basic imports and some settings. In a real example, these settings would come from flags or other model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tCM4kgp6TsZ"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "from pprint import pformat, pprint\n",
    "from typing import Any, Mapping\n",
    "\n",
    "from clu import metrics\n",
    "from clu import parameter_overview\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "from jax.experimental.layout import Format\n",
    "from jax.experimental.layout import Layout\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh\n",
    "from jax.sharding import NamedSharding\n",
    "from jax.sharding import PartitionSpec as P\n",
    "from jax_tpu_embedding.sparsecore.examples.models.shakespeare import dataset as shakespeare_data\n",
    "from jax_tpu_embedding.sparsecore.lib.fdo import file_fdo_client\n",
    "from jax_tpu_embedding.sparsecore.lib.nn import embedding\n",
    "from jax_tpu_embedding.sparsecore.lib.nn import embedding_spec\n",
    "from jax_tpu_embedding.sparsecore.utils import utils\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "Nested = embedding.Nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1024  # Maximum number of unique words.\n",
    "GLOBAL_BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.005\n",
    "SEQ_LEN = 16  # Sequence length of context words.\n",
    "NUM_TABLES = 1  # Number of tables to create.\n",
    "NUM_STEPS = 100\n",
    "NUM_EPOCHS = 1\n",
    "EMBEDDING_SIZE = 8\n",
    "EMBEDDING_INIT = 'normal'\n",
    "LOG_FREQUENCY = 10\n",
    "LOSS_RESET_FREQUENCY = 10  # Number of steps to average loss over.\n",
    "CHECKPOINT_DIR = None  # If set, checkpoints will be written to the directory.\n",
    "CHECKPOINT_INTERVAL = 500\n",
    "CHECKPOINT_RESUME = True\n",
    "CHECKPOINT_MAX_TO_KEEP = 5\n",
    "FDO_DIR = '/tmp'\n",
    "FDO_FREQUENCY = 15  # How frequently, in steps, to update FDO stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDLV4Llsy4D5"
   },
   "outputs": [],
   "source": [
    "# Device and cluster configuration\n",
    "local_devices = jax.local_devices()\n",
    "global_devices = jax.devices()\n",
    "num_global_devices = len(global_devices)\n",
    "num_local_devices = len(local_devices)\n",
    "num_sc_per_device = utils.num_sparsecores_per_device(global_devices[0])\n",
    "num_processes = jax.process_count()\n",
    "process_id = jax.process_index()\n",
    "\n",
    "# PartitionSpecs for the model and embedding tables.\n",
    "pd = P('device')  # Device sharding.\n",
    "pe = P('device', None)  # PartitionSpec for embedding tables.\n",
    "\n",
    "# Create the global mesh and shardings.\n",
    "global_mesh = Mesh(np.array(global_devices), axis_names=['device'])\n",
    "global_sharding = NamedSharding(global_mesh, pd)\n",
    "global_emb_sharding = NamedSharding(global_mesh, pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1753393830717,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "7joXIFOr5IUx",
    "outputId": "6161e348-ae50-4dd1-f820-006e2e522c30"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f'num devices: local = {num_local_devices}, global = {num_global_devices}'\n",
    ")\n",
    "print(f'process_id = {process_id}, num_processes = {num_processes}')\n",
    "print(f'local_devices = {pformat(local_devices)}')\n",
    "print(f'global_devices = {pformat(global_devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhBBaiB6wgxi"
   },
   "outputs": [],
   "source": [
    "# Define Train State and Metrics classes.\n",
    "@flax.struct.dataclass\n",
    "class TrainState:\n",
    "  \"\"\"State of the model and the training.\n",
    "\n",
    "  This includes parameters, statistics and optimizer.\n",
    "  \"\"\"\n",
    "\n",
    "  params: Any\n",
    "  opt_state: optax.OptState\n",
    "\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class TrainMetrics(metrics.Collection):\n",
    "  # train_accuracy: metrics.Accuracy\n",
    "  # learning_rate: metrics.LastValue.from_output(\"learning_rate\")\n",
    "  train_loss: metrics.Average.from_output('loss')\n",
    "  train_loss_std: metrics.Std.from_output('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5tNc0jlax7F"
   },
   "source": [
    "# The Shakespeare Model\n",
    "\n",
    "We're basing this on the example  model in [.../examples/models/shakespeare/model.py](https://github.com/jax-ml/jax-tpu-embedding/blob/main/jax_tpu_embedding/sparsecore/examples/models/shakespeare/model.py)\n",
    "\n",
    "The Shakespeare model consists of an embedding layer and two dense layers. This Flax model takes the already computed embedding activations which are of length `embedding_size`. Since any word may be predicted, the output of the dense model is the same size as the embedding vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vbwtIaXbKSN"
   },
   "outputs": [],
   "source": [
    "# Flax Model\n",
    "class Model(nn.Module):\n",
    "  \"\"\"A simple model that predicts the next word in a sequence of words.\n",
    "\n",
    "  Attributes:\n",
    "    global_batch_size: The number of examples in the global batch.\n",
    "    vocab_size: The number of unique words in the vocabulary.\n",
    "    seq_len: The length of the sequences in the global batch.\n",
    "    embedding_size: The dimension of the embedding vectors.\n",
    "    table_name: The name of the embedding table.\n",
    "    feature_name: The name of the embedding feature.\n",
    "  \"\"\"\n",
    "\n",
    "  global_batch_size: int\n",
    "  vocab_size: int\n",
    "  seq_len: int\n",
    "  embedding_size: int\n",
    "  table_name: str = 'shakespeare_table'\n",
    "  feature_name: str = 'shakespeare_feature'\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, emb_activations: Mapping[str, jax.Array]):\n",
    "    # Unpack the activations.\n",
    "    x = emb_activations[self.feature_name]\n",
    "    x = jnp.reshape(x, (x.shape[0], -1))\n",
    "    # Apply the model.\n",
    "    x = nn.Dense(self.embedding_size)(x)\n",
    "    x = nn.Dense(self.vocab_size)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    global_batch_size=GLOBAL_BATCH_SIZE,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    seq_len=SEQ_LEN,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DLb2j-Zrcpb"
   },
   "source": [
    "## The Loss Function\n",
    "\n",
    "Here we define a simple loss function for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "je_FnuTurYvI"
   },
   "outputs": [],
   "source": [
    "# The loss function\n",
    "def loss_fn(\n",
    "    model: nn.Module,\n",
    "    params: Any,\n",
    "    emb_activations: Mapping[str, jax.Array],\n",
    "    labels: jax.Array,\n",
    ") -> tuple[jax.Array, jax.Array]:\n",
    "  \"\"\"Applies the embedding activations to model and returns loss.\n",
    "\n",
    "  Args:\n",
    "    model: The model being trained.\n",
    "    params: The parameters of the model.\n",
    "    emb_activations: The embedding activations that will be applied.\n",
    "    labels: The integer labels corresponding to the embedding activations.\n",
    "\n",
    "  Returns:\n",
    "    The loss.\n",
    "  \"\"\"\n",
    "  logits = model.apply(params, emb_activations)\n",
    "  xentropy = optax.softmax_cross_entropy_with_integer_labels(\n",
    "      logits=logits, labels=labels\n",
    "  )\n",
    "  return jnp.mean(xentropy), logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xOztCeTpS0f"
   },
   "source": [
    "# Embedding API: TableSpec and FeatureSpec creation\n",
    "\n",
    "A <project:#TableSpec> describes a single embedding table which will be used by one or more sparse features in the model. We configure it's size (vocabulary size and embedding dimension), initialization method, and the optimizer to use for the weight update. The `combiner` parameter determines how multivalent features are combined into a single output activation. The `max_ids_per_partition` and `max_unique_ids_per_partition` parameters are described in <project:../parameters.rst>.\n",
    "\n",
    "A <project:#FeatureSpec> describes the batch input data for a given feature. We configure which table to use for the embedding lookup and the input and output shapes. In this example we're using a sequence of `SEQ_LEN` words for this feature so the input data has `GLOBAL_BATCH_SIZE * SEQ_LEN` tokens.\n",
    "\n",
    "In general, models may have multiple tables and multiple features. Furthermore, multiple features may share a single embedding table.\n",
    "\n",
    "More information on embedding specifications can be found in <project:../embedding.rst>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1753393831432,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "znYOZoBgpSGU",
    "outputId": "72c0827f-0afb-4c4b-9cb2-6e5f98dc82ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TableSpec(name='shakespeare_table',\n",
      "          vocabulary_size=1024,\n",
      "          embedding_dim=8,\n",
      "          initializer=<function zeros at 0x70b5cb9c5da0>,\n",
      "          optimizer=SGDOptimizerSpec(),\n",
      "          combiner='sum',\n",
      "          max_ids_per_partition=64,\n",
      "          max_unique_ids_per_partition=64,\n",
      "          suggested_coo_buffer_size_per_device=None,\n",
      "          quantization_config=None,\n",
      "          _stacked_table_spec=None,\n",
      "          _setting_in_stack=TableSettingInStack(stack_name='shakespeare_table',\n",
      "                                                padded_vocab_size=1024,\n",
      "                                                padded_embedding_dim=8,\n",
      "                                                row_offset_in_shard=0,\n",
      "                                                shard_rotation=0))\n"
     ]
    }
   ],
   "source": [
    "# TableSpec\n",
    "table_spec = embedding_spec.TableSpec(\n",
    "    vocabulary_size=model.vocab_size,\n",
    "    embedding_dim=model.embedding_size,\n",
    "    initializer=jax.nn.initializers.zeros,\n",
    "    optimizer=embedding_spec.SGDOptimizerSpec(),\n",
    "    combiner='sum',\n",
    "    name=model.table_name,\n",
    "    max_ids_per_partition=64,\n",
    "    max_unique_ids_per_partition=64,\n",
    ")\n",
    "pprint(table_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1753393831651,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "Gsz61swTq7iG",
    "outputId": "789ec0a2-4bba-44a5-992c-e2df1237443c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureSpec(name='shakespeare_feature',\n",
      "            table_spec=TableSpec(name='shakespeare_table',\n",
      "                                 vocabulary_size=1024,\n",
      "                                 embedding_dim=8,\n",
      "                                 initializer=<function zeros at 0x70b5cb9c5da0>,\n",
      "                                 optimizer=SGDOptimizerSpec(),\n",
      "                                 combiner='sum',\n",
      "                                 max_ids_per_partition=64,\n",
      "                                 max_unique_ids_per_partition=64,\n",
      "                                 suggested_coo_buffer_size_per_device=None,\n",
      "                                 quantization_config=None,\n",
      "                                 _stacked_table_spec=StackedTableSpec(stack_name='shakespeare_table',\n",
      "                                                                      stack_vocab_size=1024,\n",
      "                                                                      stack_embedding_dim=8,\n",
      "                                                                      optimizer=SGDOptimizerSpec(),\n",
      "                                                                      combiner='sum',\n",
      "                                                                      total_sample_count=np.int64(512),\n",
      "                                                                      max_ids_per_partition=64,\n",
      "                                                                      max_unique_ids_per_partition=64,\n",
      "                                                                      suggested_coo_buffer_size_per_device=None,\n",
      "                                                                      quantization_config=None),\n",
      "                                 _setting_in_stack=TableSettingInStack(stack_name='shakespeare_table',\n",
      "                                                                       padded_vocab_size=1024,\n",
      "                                                                       padded_embedding_dim=8,\n",
      "                                                                       row_offset_in_shard=0,\n",
      "                                                                       shard_rotation=0)),\n",
      "            input_shape=(512, 1),\n",
      "            output_shape=(512, 8),\n",
      "            _id_transformation=FeatureIdTransformation(row_offset=0,\n",
      "                                                       col_offset=0,\n",
      "                                                       col_shift=0))\n"
     ]
    }
   ],
   "source": [
    "# FeatureSpec\n",
    "feature_spec = embedding_spec.FeatureSpec(\n",
    "    table_spec=table_spec,\n",
    "    input_shape=(model.global_batch_size * model.seq_len, 1),\n",
    "    output_shape=(\n",
    "        model.global_batch_size * model.seq_len,\n",
    "        model.embedding_size,\n",
    "    ),\n",
    "    name=model.feature_name,\n",
    ")\n",
    "feature_specs = nn.FrozenDict({model.feature_name: feature_spec})\n",
    "\n",
    "# This call will take care of stacking features and other automatable\n",
    "# configuration settings.\n",
    "embedding.prepare_feature_specs_for_training(\n",
    "    feature_specs,\n",
    "    global_device_count=num_global_devices,\n",
    "    num_sc_per_device=num_sc_per_device,\n",
    ")\n",
    "for fs in feature_specs.values():\n",
    "  pprint(fs)\n",
    "\n",
    "table_specs = {\n",
    "    f.table_spec.name: f.table_spec for f in jax.tree.leaves(feature_specs)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-X3pd4XS3Z-_"
   },
   "source": [
    "# Model Initialization\n",
    "\n",
    "Here, we initialize the model. We start by creating a zero-array for the embedding activations which are then used as inputs to initialize the dense model. We also initialize the emebdding tables with the initialization functions specified in the `TableSpec`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1753393831892,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "nF_NmP8dwjoK",
    "outputId": "ffa5ae34-556f-46ec-d5b5-40adde402a94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params =\n",
      " +-----------------------+-----------+---------+-------+---------+--------+\n",
      "| Name                  | Shape     | Dtype   | Size  | Mean    | Std    |\n",
      "+-----------------------+-----------+---------+-------+---------+--------+\n",
      "| params/Dense_0/bias   | (8,)      | float32 | 8     | 0.0     | 0.0    |\n",
      "| params/Dense_0/kernel | (128, 8)  | float32 | 1,024 | 0.00226 | 0.0887 |\n",
      "| params/Dense_1/bias   | (1024,)   | float32 | 1,024 | 0.0     | 0.0    |\n",
      "| params/Dense_1/kernel | (8, 1024) | float32 | 8,192 | 0.00592 | 0.354  |\n",
      "+-----------------------+-----------+---------+-------+---------+--------+\n",
      "Total: 10,248 -- 40,992 bytes\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and training state.\n",
    "\n",
    "# Global embedding activations. We can change this to local arrays and use\n",
    "# make_array_from_single_device_arrays as above.\n",
    "init_emb_activations = {\n",
    "    model.feature_name: jnp.zeros((\n",
    "        model.global_batch_size,\n",
    "        model.seq_len,\n",
    "        1,\n",
    "        model.embedding_size,\n",
    "    ))\n",
    "}\n",
    "rng = jax.random.key(42)\n",
    "params = model.init(rng, init_emb_activations)\n",
    "print(f'params =\\n {parameter_overview.get_parameter_overview(params)}')\n",
    "\n",
    "optimizer = optax.adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "train_state = TrainState(\n",
    "    params=params,\n",
    "    opt_state=optimizer.init(params),\n",
    ")\n",
    "\n",
    "emb_variables = embedding.init_embedding_variables(\n",
    "    jax.random.key(13), table_specs, global_emb_sharding, num_sc_per_device\n",
    ")\n",
    "\n",
    "# Outsharding the embedding variables. Note that arrays on the SparseCore are\n",
    "# expected to have row major layout while the default layout is column major.\n",
    "# Here, we explicitly set the layout to row major.\n",
    "emb_var_outsharding = utils.embedding_table_format(\n",
    "    global_emb_sharding.mesh, global_emb_sharding.spec\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LS_rqL441ht_"
   },
   "source": [
    "# Model Data\n",
    "\n",
    "For this simple model, we preload all the data into fixed arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1753393832080,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "e0gTnAzIxjku",
    "outputId": "9680d8d5-12ac-4b8b-af39-fa0f1680f06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch sizes: global=32, local=32, device=8\n",
      "word_ids len = 454\n",
      "feature_batches len = 1000\n",
      "feature_batches[0] shape = (32, 16, 1)\n",
      "label_batches len = 1000\n",
      "label_batches[0] shape = (32,)\n"
     ]
    }
   ],
   "source": [
    "# Preload the model data\n",
    "\n",
    "# Note: The input processor expects 2-d lookups, so we scale-up the batch\n",
    "# size and reshape the results.\n",
    "\n",
    "local_batch_size = GLOBAL_BATCH_SIZE // num_processes\n",
    "device_batch_size = GLOBAL_BATCH_SIZE // num_global_devices\n",
    "print(\n",
    "    f'batch sizes: global={GLOBAL_BATCH_SIZE}, local={local_batch_size},'\n",
    "    f' device={device_batch_size}'\n",
    ")\n",
    "\n",
    "per_sc_vocab_size = VOCAB_SIZE // num_sc_per_device\n",
    "if per_sc_vocab_size < 8 or per_sc_vocab_size % 8 != 0:\n",
    "  raise ValueError(\n",
    "      'Vocabulary size must be a multiple of 8 per SC: VOCAB_SIZE ='\n",
    "      f' {VOCAB_SIZE}, num_scs = {num_sc_per_device}'\n",
    "  )\n",
    "\n",
    "word_ids = shakespeare_data.load_shakespeare(VOCAB_SIZE)\n",
    "print(f'word_ids len = {len(word_ids)}')\n",
    "feature_batches, label_batches = shakespeare_data.word_id_batches(\n",
    "    word_ids,\n",
    "    NUM_STEPS,\n",
    "    GLOBAL_BATCH_SIZE,\n",
    "    SEQ_LEN,\n",
    "    NUM_TABLES,\n",
    ")\n",
    "feature_batches = feature_batches['words_0']\n",
    "print(f'feature_batches len = {len(feature_batches)}')\n",
    "print(f'feature_batches[0] shape = {feature_batches[0].shape}')\n",
    "print(f'label_batches len = {len(label_batches)}')\n",
    "print(f'label_batches[0] shape = {label_batches[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oecmtkqP2hxL"
   },
   "source": [
    "# The Training Step Function\n",
    "\n",
    "The training step function takes a step by\n",
    "1. Performing the embedding lookup.\n",
    "2. Passing the activations as inputs to the dense model to:\n",
    "  - (a) compute the loss and dense gradients and,\n",
    "  - (b) update the dense weights.\n",
    "3. Using the back propagated dense gradients to update the embedding weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKD6U2ah2JbA"
   },
   "outputs": [],
   "source": [
    "@partial(\n",
    "    jax.jit,\n",
    "    static_argnums=(0, 1, 2, 3),\n",
    "    out_shardings=(\n",
    "        None,\n",
    "        None,\n",
    "        emb_var_outsharding,\n",
    "    ),\n",
    "    donate_argnums=(6),\n",
    ")\n",
    "def train_step_fn(\n",
    "    mesh: jax.sharding.Mesh,\n",
    "    model: nn.Module,\n",
    "    optimizer,\n",
    "    feature_specs,\n",
    "    train_state: TrainState,\n",
    "    preprocessed_inputs,\n",
    "    emb_variables,\n",
    "    labels,\n",
    ") -> tuple[TrainState, TrainMetrics, Nested[jax.Array]]:\n",
    "  \"\"\"Performs a single training step at the chip level.\"\"\"\n",
    "\n",
    "  ##############################################################################\n",
    "  # Sparse forward pass - embedding lookup.\n",
    "  ##############################################################################\n",
    "  tpu_sparse_dense_matmul = partial(\n",
    "      embedding.tpu_sparse_dense_matmul,\n",
    "      global_device_count=num_global_devices,\n",
    "      feature_specs=feature_specs,\n",
    "      sharding_strategy='MOD',\n",
    "  )\n",
    "  tpu_sparse_dense_matmul = jax.shard_map(\n",
    "      tpu_sparse_dense_matmul,\n",
    "      mesh=mesh,\n",
    "      in_specs=(pd, pe),\n",
    "      out_specs=pd,\n",
    "      check_vma=False,\n",
    "  )\n",
    "  emb_act = tpu_sparse_dense_matmul(\n",
    "      preprocessed_inputs,\n",
    "      emb_variables,\n",
    "  )\n",
    "\n",
    "  ##############################################################################\n",
    "  # Dense forward + backward pass.\n",
    "  ##############################################################################\n",
    "  emb_act = jax.tree_util.tree_map(\n",
    "      lambda x: jnp.reshape(x, (model.global_batch_size, -1)), emb_act\n",
    "  )\n",
    "  loss_grad_fn = jax.value_and_grad(\n",
    "      partial(loss_fn, model), argnums=(0, 1), has_aux=True\n",
    "  )\n",
    "\n",
    "  (loss, logits), (dense_grad, emb_grad) = loss_grad_fn(\n",
    "      train_state.params, emb_act, labels\n",
    "  )\n",
    "\n",
    "  updates, new_opt_state = optimizer.update(\n",
    "      dense_grad, train_state.opt_state, train_state.params\n",
    "  )\n",
    "  new_params = optax.apply_updates(train_state.params, updates)\n",
    "\n",
    "  emb_grad = jax.tree_util.tree_map(\n",
    "      lambda x: jnp.reshape(x, (-1, model.embedding_size)), emb_grad\n",
    "  )\n",
    "\n",
    "  ##############################################################################\n",
    "  # Sparse backward pass - embedding update.\n",
    "  ##############################################################################\n",
    "  tpu_sparse_dense_matmul_grad = partial(\n",
    "      embedding.tpu_sparse_dense_matmul_grad,\n",
    "      feature_specs=feature_specs,\n",
    "      sharding_strategy='MOD',\n",
    "  )\n",
    "  tpu_sparse_dense_matmul_grad = jax.shard_map(\n",
    "      tpu_sparse_dense_matmul_grad,\n",
    "      mesh=mesh,\n",
    "      in_specs=(pd, pd, pe),\n",
    "      out_specs=pe,\n",
    "      check_vma=False,\n",
    "  )\n",
    "  emb_variables = tpu_sparse_dense_matmul_grad(\n",
    "      emb_grad,\n",
    "      preprocessed_inputs,\n",
    "      emb_variables,\n",
    "  )\n",
    "\n",
    "  train_state = train_state.replace(params=new_params, opt_state=new_opt_state)\n",
    "\n",
    "  metrics_update = TrainMetrics.single_from_model_output(\n",
    "      loss=loss,\n",
    "      logits=logits,\n",
    "      labels=labels,\n",
    "  )\n",
    "\n",
    "  return train_state, metrics_update, emb_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iW_z5yDsY_o"
   },
   "source": [
    "# FDO Configuration\n",
    "\n",
    "FDO (Feedback Directed Optimization) is an optional feature where we use\n",
    "batch statistics to update table limits (`max_ids_per_partition`, etc.). The FDO\n",
    "client is a generic interface and clients can either use provided\n",
    "implementations or create their own that intergrates with their infrastructure.\n",
    "When the FDO stats are used to update the limits on the `feature_specs`,\n",
    "`jax.jit` triggers a recompilation.\n",
    "\n",
    "In this example, we use a simple file-based implementation. Each host writes its\n",
    "own stats on `publish()` and stats from all hosts are merged during `load()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1753393832328,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "3y2BOPnWscwl",
    "outputId": "7fa6cf97-c578-4590-b47f-e8a607764106"
   },
   "outputs": [],
   "source": [
    "out_path = os.path.join(FDO_DIR, 'fdo_dump')\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "print(f'FDO storage path: {out_path}')\n",
    "fdo_client = file_fdo_client.NPZFileFDOClient(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzFVKzXg4UvT"
   },
   "source": [
    "# The Training Loop\n",
    "\n",
    "The training loop is a simple Python `for`-loop. For each step, we preprocess the input batch data (on the host / CPU). In doing so, we capture the batch statistics which are used by FDO. Then we take a training step using the jitted `train_step_fn`. We finish each step by processing the training metrics and FDO stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5683,
     "status": "ok",
     "timestamp": 1753393838127,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "Yili5lon4Qeh",
    "outputId": "785457f3-783c-4e8e-c12b-e4b2716bb0fd"
   },
   "outputs": [],
   "source": [
    "parameter_overview.log_parameter_overview(train_state.params)\n",
    "train_metrics = None\n",
    "step = 0\n",
    "# Track these separately for testing.\n",
    "test_loss = None\n",
    "test_step_count = 0\n",
    "for features, labels in zip(feature_batches, label_batches):\n",
    "  step += 1\n",
    "  test_step_count += 1\n",
    "  print(f'Step {step}')\n",
    "  ##############################################################################\n",
    "  # SC input processing.\n",
    "  ##############################################################################\n",
    "  # These are currently global batches so each task needs to offset into\n",
    "  # the data for it's local slice.\n",
    "  labels = labels[\n",
    "      process_id * local_batch_size : (process_id + 1) * local_batch_size\n",
    "  ]\n",
    "  labels = jax.make_array_from_process_local_data(global_sharding, labels)\n",
    "\n",
    "  # Each input preprocessing processes the current process's slice of the\n",
    "  # global batch.\n",
    "  features = features[\n",
    "      process_id * local_batch_size : (process_id + 1) * local_batch_size\n",
    "  ]\n",
    "  features = np.reshape(features, (-1, 1))\n",
    "  feature_weights = np.ones(features.shape, dtype=np.float32)\n",
    "\n",
    "  # Pack the features into a tree structure.\n",
    "  feature_structure = jax.tree.structure(feature_specs)\n",
    "  features = jax.tree_util.tree_unflatten(feature_structure, [features])\n",
    "  feature_weights = jax.tree_util.tree_unflatten(\n",
    "      feature_structure, [feature_weights]\n",
    "  )\n",
    "\n",
    "  ##############################################################################\n",
    "  # Preprocess the inputs and build JAX global views of the data.\n",
    "  ##############################################################################\n",
    "  make_global_view = lambda x: jax.tree.map(\n",
    "      lambda y: jax.make_array_from_process_local_data(global_sharding, y),\n",
    "      x,\n",
    "  )\n",
    "  preprocessed_inputs, step_stats = (\n",
    "      embedding.preprocess_sparse_dense_matmul_input(\n",
    "          features,\n",
    "          feature_weights,\n",
    "          feature_specs,\n",
    "          local_device_count=global_mesh.local_mesh.size,\n",
    "          global_device_count=global_mesh.size,\n",
    "          num_sc_per_device=num_sc_per_device,\n",
    "          sharding_strategy='MOD',\n",
    "          batch_number=step,\n",
    "      )\n",
    "  )\n",
    "  preprocessed_inputs = make_global_view(preprocessed_inputs)\n",
    "  fdo_client.record(step_stats)\n",
    "\n",
    "  ##############################################################################\n",
    "  # Combined: SC forward, TC, SC backward\n",
    "  ##############################################################################\n",
    "  # We capture the logging here so you can see when XLA compilation is\n",
    "  # triggered. This should happen following the first FDO update to the limits.\n",
    "  train_state, metrics_update, emb_variables = train_step_fn(\n",
    "      global_mesh,\n",
    "      model,\n",
    "      optimizer,\n",
    "      feature_specs,\n",
    "      train_state,\n",
    "      preprocessed_inputs,\n",
    "      emb_variables,\n",
    "      labels,\n",
    "  )\n",
    "\n",
    "  train_metrics = (\n",
    "      metrics_update\n",
    "      if train_metrics is None\n",
    "      else train_metrics.merge(metrics_update)\n",
    "  )\n",
    "\n",
    "  if step % LOG_FREQUENCY == 0:\n",
    "    m = train_metrics.compute()\n",
    "    test_loss = m['train_loss']\n",
    "    print(f'Step {step}: Loss = {m['train_loss']}')\n",
    "    parameter_overview.log_parameter_overview(train_state.params)\n",
    "\n",
    "  if (step + 1) % LOSS_RESET_FREQUENCY == 0:\n",
    "    train_metrics = None\n",
    "\n",
    "  if step % FDO_FREQUENCY == 0:\n",
    "    print(f'Refreshing FDO stats after step {step}')\n",
    "    fdo_client.publish()\n",
    "    loaded_stats = fdo_client.load()\n",
    "    embedding.update_preprocessing_parameters(\n",
    "        feature_specs, loaded_stats, num_sc_per_device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1753393838283,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "utG-AIxi6QzM",
    "outputId": "1a5f4403-5add-40c6-f9b5-6b26d304ae36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss = 0.0021\n",
      "test_step_count = 1000\n"
     ]
    }
   ],
   "source": [
    "# Print final loss and step count for the colab test.\n",
    "print(f'test_loss = {test_loss:.4f}')\n",
    "print(f'test_step_count = {test_step_count}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
