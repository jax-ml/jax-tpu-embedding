{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_wsiwhfwwmW"
   },
   "source": [
    "# The JAX SC Shakespeare Example - Flax\n",
    "\n",
    "_An example Shakespeare model that uses the Flax layer interface of the SparseCore embedding API_\n",
    "\n",
    "Using some text from the Shakespeare corpus, we're going to construct a Flax model that predicts the next word for a given sequence of words. This tutorial is based on the working example in [.../lib/flax/linen/tests/autograd_test.py](https://github.com/jax-ml/jax-tpu-embedding/blob/main/jax_tpu_embedding/sparsecore/lib/flax/linen/tests/autograd_test.py) using the [SparseCoreEmbed](#jax_tpu_embedding.sparsecore.lib.flax.linen.embed.SparseCoreEmbed) class.\n",
    "\n",
    "In this example we'll walk through the construction and configuration of the model paying particular attention to the embedding aspects.\n",
    "\n",
    "This example domonstrates the following features of the JAX SC API:\n",
    "- Input preprocessing\n",
    "- <project:#TableSpec> and <project:#FeatureSpec> creation\n",
    "- Model definition, including the embedding Flax layer\n",
    "- Execution of the model in a simple training loop\n",
    "- FDO (feedback directed optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52eYRZZCaIep"
   },
   "source": [
    "# Front matter\n",
    "\n",
    "We start off with some basic imports and some settings. In a real example, these settings would come from flags or other model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tCM4kgp6TsZ"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "from pprint import pformat, pprint\n",
    "from typing import Any\n",
    "\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh\n",
    "from jax.sharding import NamedSharding\n",
    "from jax.sharding import PartitionSpec as P\n",
    "from jax_tpu_embedding.sparsecore.examples.models.shakespeare import dataset as shakespeare_data\n",
    "from jax_tpu_embedding.sparsecore.lib.fdo import file_fdo_client\n",
    "from jax_tpu_embedding.sparsecore.lib.flax.linen import embed\n",
    "from jax_tpu_embedding.sparsecore.lib.flax.linen import embed_optimizer\n",
    "from jax_tpu_embedding.sparsecore.lib.nn import embedding\n",
    "from jax_tpu_embedding.sparsecore.lib.nn import embedding_spec\n",
    "from jax_tpu_embedding.sparsecore.utils import utils\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "Nested = embedding.Nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQh5j0Ic634e"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 2048  # Maximum number of unique words.\n",
    "GLOBAL_BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.005\n",
    "SEQ_LEN = 16  # Sequence length of context words.\n",
    "NUM_TABLES = 1  # Number of tables to create.\n",
    "NUM_STEPS = 100\n",
    "NUM_EPOCHS = 1\n",
    "EMBEDDING_SIZE = 16\n",
    "EMBEDDING_INIT = 'normal'\n",
    "LOG_FREQUENCY = 10\n",
    "LOSS_RESET_FREQUENCY = 10  # Number of steps to average loss over.\n",
    "CHECKPOINT_DIR = None  # If set, checkpoints will be written to the directory.\n",
    "CHECKPOINT_INTERVAL = 500\n",
    "CHECKPOINT_RESUME = True\n",
    "CHECKPOINT_MAX_TO_KEEP = 5\n",
    "FDO_DIR = '/tmp'\n",
    "FDO_FREQUENCY = 15  # How frequently, in steps, to update FDO stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDLV4Llsy4D5"
   },
   "outputs": [],
   "source": [
    "# Device and cluster configuration\n",
    "local_devices = jax.local_devices()\n",
    "global_devices = jax.devices()\n",
    "num_global_devices = len(global_devices)\n",
    "num_local_devices = len(local_devices)\n",
    "num_sc_per_device = utils.num_sparsecores_per_device(global_devices[0])\n",
    "num_processes = jax.process_count()\n",
    "process_id = jax.process_index()\n",
    "\n",
    "_SHARDING_AXIS = 'device'\n",
    "\n",
    "# PartitionSpecs for the model and embedding tables.\n",
    "pd = P(_SHARDING_AXIS)  # Device sharding.\n",
    "\n",
    "# Create the global mesh and shardings.\n",
    "global_mesh = Mesh(np.array(global_devices), axis_names=[_SHARDING_AXIS])\n",
    "data_sharding = NamedSharding(global_mesh, pd)\n",
    "\n",
    "local_batch_size = GLOBAL_BATCH_SIZE // num_processes\n",
    "device_batch_size = GLOBAL_BATCH_SIZE // num_global_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1759165866069,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "CPoxj6QV3plm",
    "outputId": "f675dc47-fdbb-4191-fd16-0e736cb2bc13"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f'num devices: local = {num_local_devices}, global = {num_global_devices}'\n",
    ")\n",
    "print(f'process_id = {process_id}, num_processes = {num_processes}')\n",
    "print(f'local_devices = {pformat(local_devices)}')\n",
    "print(f'global_devices = {pformat(global_devices)}')\n",
    "print(\n",
    "    f'batch sizes: global={GLOBAL_BATCH_SIZE}, local={local_batch_size},'\n",
    "    f' device={device_batch_size}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xOztCeTpS0f"
   },
   "source": [
    "# Embedding API: TableSpec and FeatureSpec creation\n",
    "\n",
    "A <project:#TableSpec> describes a single embedding table which will be used by one or more sparse features in the model. We configure it's size (vocabulary size and embedding dimension), initialization method, and the optimizer to use for the weight update. The `combiner` parameter determines how multivalent features are combined into a single output activation. The `max_ids_per_partition` and `max_unique_ids_per_partition` parameters are described in <project:../parameters.rst>.\n",
    "\n",
    "A <project:#FeatureSpec> describes the batch input data for a given feature. We configure which table to use for the embedding lookup and the input and output shapes. In this example we're using a sequence of `SEQ_LEN` words for this feature so the input data has `GLOBAL_BATCH_SIZE * SEQ_LEN` tokens.\n",
    "\n",
    "In general, models may have multiple tables and multiple features. Furthermore, multiple features may share a single embedding table.\n",
    "\n",
    "More information on embedding specifications can be found in <project:../embedding.rst>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1759165866252,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "znYOZoBgpSGU",
    "outputId": "82ec69fd-7c14-4194-866d-b19da2f1f646"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TableSpec(name='shakespeare',\n",
      "          vocabulary_size=2048,\n",
      "          embedding_dim=16,\n",
      "          initializer=<function normal.<locals>.init at 0x104b9f6368e0>,\n",
      "          optimizer=AdamOptimizerSpec(),\n",
      "          combiner='sum',\n",
      "          max_ids_per_partition=64,\n",
      "          max_unique_ids_per_partition=64,\n",
      "          suggested_coo_buffer_size_per_device=None,\n",
      "          quantization_config=None,\n",
      "          _stacked_table_spec=None,\n",
      "          _setting_in_stack=TableSettingInStack(stack_name='shakespeare',\n",
      "                                                padded_vocab_size=2048,\n",
      "                                                padded_embedding_dim=16,\n",
      "                                                row_offset_in_shard=0,\n",
      "                                                shard_rotation=0))\n"
     ]
    }
   ],
   "source": [
    "# TableSpec\n",
    "table_spec = embedding_spec.TableSpec(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_SIZE,\n",
    "    initializer=jax.nn.initializers.normal(),\n",
    "    optimizer=embedding_spec.AdamOptimizerSpec(learning_rate=LEARNING_RATE),\n",
    "    combiner='sum',\n",
    "    name='shakespeare',\n",
    "    max_ids_per_partition=64,\n",
    "    max_unique_ids_per_partition=64,\n",
    ")\n",
    "pprint(table_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1759165866447,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "Gsz61swTq7iG",
    "outputId": "16b54dfa-4c5b-4d55-ec62-d74ef5e1bd27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureSpec(name='shakespeare_feature',\n",
      "            table_spec=TableSpec(name='shakespeare',\n",
      "                                 vocabulary_size=2048,\n",
      "                                 embedding_dim=16,\n",
      "                                 initializer=<function normal.<locals>.init at 0x104b9f6368e0>,\n",
      "                                 optimizer=AdamOptimizerSpec(),\n",
      "                                 combiner='sum',\n",
      "                                 max_ids_per_partition=64,\n",
      "                                 max_unique_ids_per_partition=64,\n",
      "                                 suggested_coo_buffer_size_per_device=None,\n",
      "                                 quantization_config=None,\n",
      "                                 _stacked_table_spec=StackedTableSpec(stack_name='shakespeare',\n",
      "                                                                      stack_vocab_size=2048,\n",
      "                                                                      stack_embedding_dim=16,\n",
      "                                                                      optimizer=AdamOptimizerSpec(),\n",
      "                                                                      combiner='sum',\n",
      "                                                                      total_sample_count=np.int64(4096),\n",
      "                                                                      max_ids_per_partition=64,\n",
      "                                                                      max_unique_ids_per_partition=64,\n",
      "                                                                      suggested_coo_buffer_size_per_device=None,\n",
      "                                                                      quantization_config=None),\n",
      "                                 _setting_in_stack=TableSettingInStack(stack_name='shakespeare',\n",
      "                                                                       padded_vocab_size=2048,\n",
      "                                                                       padded_embedding_dim=16,\n",
      "                                                                       row_offset_in_shard=0,\n",
      "                                                                       shard_rotation=0)),\n",
      "            input_shape=(4096, 1),\n",
      "            output_shape=(4096, 16),\n",
      "            _id_transformation=FeatureIdTransformation(row_offset=0,\n",
      "                                                       col_offset=0,\n",
      "                                                       col_shift=0))\n"
     ]
    }
   ],
   "source": [
    "# FeatureSpec\n",
    "feature_spec = embedding_spec.FeatureSpec(\n",
    "    table_spec=table_spec,\n",
    "    input_shape=(GLOBAL_BATCH_SIZE * SEQ_LEN, 1),\n",
    "    output_shape=(\n",
    "        GLOBAL_BATCH_SIZE * SEQ_LEN,\n",
    "        EMBEDDING_SIZE,\n",
    "    ),\n",
    "    name='shakespeare_feature',\n",
    ")\n",
    "feature_specs = nn.FrozenDict({feature_spec.name: feature_spec})\n",
    "\n",
    "# This call will take care of stacking features and other automatable\n",
    "# configuration settings.\n",
    "embedding.prepare_feature_specs_for_training(\n",
    "    feature_specs,\n",
    "    global_device_count=num_global_devices,\n",
    "    num_sc_per_device=num_sc_per_device,\n",
    ")\n",
    "feature_specs = nn.FrozenDict(feature_specs)\n",
    "\n",
    "for fs in feature_specs.values():\n",
    "  pprint(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5tNc0jlax7F"
   },
   "source": [
    "# The Shakespeare Model\n",
    "\n",
    "We're basing this on the example  model in [.../examples/models/shakespeare/flax_model.py](https://github.com/jax-ml/jax-tpu-embedding/blob/main/jax_tpu_embedding/sparsecore/examples/models/shakespeare/flax_model.py)\n",
    "\n",
    "The Shakespeare model consists of an embedding layer and two dense layers. As input, the model takes preprocessed embedding inputs (of type `EmbeddingLookupInput`) which it uses to compute the embedding activations using the generic `SparseCoreEmbed` Flax layer. Since any word may be predicted, the output of the dense model is the same size as the embedding vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vbwtIaXbKSN"
   },
   "outputs": [],
   "source": [
    "# Flax Model with SparseCore embedding layer.\n",
    "class Model(nn.Module):\n",
    "  \"\"\"Shakespeare model using embedding layer.\"\"\"\n",
    "\n",
    "  feature_specs: Nested[embedding_spec.FeatureSpec]\n",
    "  global_batch_size: int\n",
    "  vocab_size: int\n",
    "  seq_len: int\n",
    "  embedding_size: int\n",
    "  feature_name: str = 'shakespeare_feature'\n",
    "  mesh: jax.sharding.Mesh | None = None\n",
    "  sharding_axis: str = 'sparsecore_sharding'\n",
    "\n",
    "  def add_sharding_constraint(self, x: jax.Array, names: tuple[str | None]):\n",
    "    # Add a sharding constraint to the array.\n",
    "    #\n",
    "    # Add a sharding constraint to the array to ensure that the sharding\n",
    "    # information is not lost during compilation. This may not be necessary but\n",
    "    # it helps SPMD and ensures that the sharding information is as expected.\n",
    "    #\n",
    "    # Args:\n",
    "    #   x: The array to add the sharding constraint to.\n",
    "    #   names: The mesh axes for the partition spec.\n",
    "    #\n",
    "    # Returns:\n",
    "    #   The array with the sharding constraint added.\n",
    "    return jax.lax.with_sharding_constraint(\n",
    "        x,\n",
    "        jax.sharding.NamedSharding(\n",
    "            self.mesh, jax.sharding.PartitionSpec(*names)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, embedding_lookup_inputs: embedding.PreprocessedInput):\n",
    "    # Run the embedding layer.\n",
    "    x = embed.SparseCoreEmbed(\n",
    "        feature_specs=self.feature_specs,\n",
    "        mesh=self.mesh,\n",
    "        sharding_axis=self.sharding_axis,\n",
    "    )(embedding_lookup_inputs)\n",
    "\n",
    "    # Unpack the activations.\n",
    "    x = x[self.feature_name]\n",
    "    x = jnp.reshape(x, (self.global_batch_size, -1))\n",
    "    x = self.add_sharding_constraint(x, (self.sharding_axis,))\n",
    "\n",
    "    # Apply the dense portion of the model.\n",
    "    x = nn.Dense(self.embedding_size)(x)\n",
    "    x = self.add_sharding_constraint(x, (self.sharding_axis,))\n",
    "    x = nn.Dense(self.vocab_size)(x)\n",
    "    x = self.add_sharding_constraint(x, (self.sharding_axis,))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    feature_specs=feature_specs,\n",
    "    global_batch_size=GLOBAL_BATCH_SIZE,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    seq_len=SEQ_LEN,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    mesh=global_mesh,\n",
    "    sharding_axis=_SHARDING_AXIS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-X3pd4XS3Z-_"
   },
   "source": [
    "# Model Initialization\n",
    "\n",
    "Here, we initialize the model. We start by creating a zero-array for the embedding activations which are then used as inputs to initialize the dense model. We also initialize the emebdding tables with the initialization functions specified in the `TableSpec`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPZarCymk9rz"
   },
   "outputs": [],
   "source": [
    "# Process an input batch\n",
    "def process_inputs(\n",
    "    feature_specs: Nested[embedding_spec.FeatureSpec],\n",
    "    batch_number: int,\n",
    "    feature_batch: embedding.ArrayLike,\n",
    "    global_mesh: jax.sharding.Mesh,\n",
    "    data_sharding: jax.sharding.NamedSharding,\n",
    "    num_sc_per_device: int,\n",
    ") -> tuple[embedding.PreprocessedInput, embedding.SparseDenseMatmulInputStats]:\n",
    "  \"\"\"Preprocess a Shakespeare batch into PreprocessedInput and stats.\n",
    "\n",
    "  Args:\n",
    "    feature_specs: The feature specs.\n",
    "    batch_number: The batch number.\n",
    "    feature_batch: The feature batch.\n",
    "    global_mesh: The global mesh.\n",
    "    data_sharding: The NamedSharding for the data.\n",
    "    num_sc_per_device: The number of sparse cores per device.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of PreprocessedInput and SparseDenseMatmulInputStats.\n",
    "  \"\"\"\n",
    "  features = np.reshape(feature_batch, (-1, 1))\n",
    "  feature_weights = np.ones(features.shape, dtype=np.float32)\n",
    "\n",
    "  # Pack the features into a tree structure.\n",
    "  feature_structure = jax.tree.structure(feature_specs)\n",
    "  features = jax.tree_util.tree_unflatten(feature_structure, [features])\n",
    "  feature_weights = jax.tree_util.tree_unflatten(\n",
    "      feature_structure, [feature_weights]\n",
    "  )\n",
    "  processed_inputs, stats = embedding.preprocess_sparse_dense_matmul_input(\n",
    "      features,\n",
    "      feature_weights,\n",
    "      feature_specs,\n",
    "      local_device_count=global_mesh.local_mesh.size,\n",
    "      global_device_count=global_mesh.size,\n",
    "      num_sc_per_device=num_sc_per_device,\n",
    "      sharding_strategy='MOD',\n",
    "      batch_number=batch_number,\n",
    "  )\n",
    "  processed_inputs = jax.tree.map(\n",
    "      lambda x: jax.make_array_from_process_local_data(data_sharding, x),\n",
    "      processed_inputs,\n",
    "  )\n",
    "  return processed_inputs, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1759165867078,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "e0gTnAzIxjku",
    "outputId": "e4665db2-15c5-4083-aaa3-25f716f4b6e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batches len = 100\n",
      "feature_batches[0] shape = (256, 16, 1)\n",
      "label_batches len = 100\n",
      "label_batches[0] shape = (256,)\n"
     ]
    }
   ],
   "source": [
    "# Preload the model data\n",
    "word_ids = shakespeare_data.load_shakespeare(VOCAB_SIZE)\n",
    "feature_batches, label_batches = shakespeare_data.word_id_batches(\n",
    "    word_ids,\n",
    "    NUM_STEPS,\n",
    "    GLOBAL_BATCH_SIZE,\n",
    "    SEQ_LEN,\n",
    "    NUM_TABLES,\n",
    ")\n",
    "feature_batches = feature_batches['words_0']\n",
    "\n",
    "# Note: The input processor expects 2-d lookups, so we scale-up the batch\n",
    "# size and reshape the results.\n",
    "\n",
    "print(f'feature_batches len = {len(feature_batches)}')\n",
    "print(f'feature_batches[0] shape = {feature_batches[0].shape}')\n",
    "print(f'label_batches len = {len(label_batches)}')\n",
    "print(f'label_batches[0] shape = {label_batches[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfmB7gXSo1_K"
   },
   "source": [
    "## Extracting the model sharding\n",
    "\n",
    "In order to jit the model for initialization and training, we need to provide the proper sharding specification for the model and it's nested layers. In the following, we use `jax.eval_shape()` and `nn.get_sharding()` to programmatically discover this sharding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 793,
     "status": "ok",
     "timestamp": 1759165868058,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "akFZzTfDlVo3",
    "outputId": "31c8fbb0-2a16-405e-d1ef-17941776f382"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model shape\n",
      "Getting the params sharding\n",
      "Jitting init_fn\n",
      "Running the model initialization via init_fn\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and training state.\n",
    "first_model_input, unused_stats = process_inputs(\n",
    "    feature_specs,\n",
    "    -1,\n",
    "    feature_batches[0],\n",
    "    global_mesh,\n",
    "    data_sharding,\n",
    "    num_sc_per_device,\n",
    ")\n",
    "\n",
    "\n",
    "def init_fn(rng, data, model):\n",
    "  params = model.init(rng, data)\n",
    "  return params\n",
    "\n",
    "\n",
    "# Create an abstract closure to wrap the function before feeding it in\n",
    "# because `jax.eval_shape` only takes pytrees as arguments.\n",
    "print('Evaluating model shape')\n",
    "abstract_variables = jax.eval_shape(\n",
    "    functools.partial(init_fn, model=model),\n",
    "    jax.random.key(42),\n",
    "    first_model_input,\n",
    ")\n",
    "# This `params_sharding` has the same pytree structure as `params`, the output\n",
    "# of the `init_fn`.\n",
    "print('Getting the params sharding')\n",
    "params_sharding = nn.get_sharding(abstract_variables, global_mesh)\n",
    "rng_sharding = NamedSharding(global_mesh, P())\n",
    "\n",
    "print('Jitting init_fn')\n",
    "jit_init_fn = jax.jit(\n",
    "    init_fn,\n",
    "    static_argnums=(2,),\n",
    "    in_shardings=(\n",
    "        rng_sharding,\n",
    "        data_sharding,\n",
    "    ),\n",
    "    out_shardings=params_sharding,\n",
    ")\n",
    "\n",
    "print('Running the model initialization via init_fn')\n",
    "params = jit_init_fn(jax.random.key(42), first_model_input, model)\n",
    "\n",
    "# Create optimizer.\n",
    "tx = embed_optimizer.create_optimizer_for_sc_model(\n",
    "    params,\n",
    "    optax.adam(learning_rate=LEARNING_RATE),\n",
    ")\n",
    "opt_state_sharding = NamedSharding(global_mesh, P())\n",
    "opt_state = tx.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oecmtkqP2hxL"
   },
   "source": [
    "# The Training Step Function\n",
    "\n",
    "The training step is mostly a call to `model.apply()` and a loss calculation. All the details of the embedding lookup and weight update are handled by the `SparseCoreEmbed` Flax layer.\n",
    "\n",
    "We can use buffer donation on the `params` argument because the values are updated internally and the input `params` aren't used after calling the training step function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKD6U2ah2JbA"
   },
   "outputs": [],
   "source": [
    "@functools.partial(\n",
    "    jax.jit,\n",
    "    in_shardings=(\n",
    "        params_sharding,\n",
    "        data_sharding,\n",
    "        data_sharding,\n",
    "        opt_state_sharding,\n",
    "    ),\n",
    "    out_shardings=(\n",
    "        params_sharding,\n",
    "        opt_state_sharding,\n",
    "        None,\n",
    "    ),\n",
    "    donate_argnums=(0),\n",
    ")\n",
    "def train_step(\n",
    "    params: Any,\n",
    "    embedding_lookup_inputs: embedding.PreprocessedInput,\n",
    "    labels: jax.Array,\n",
    "    opt_state,\n",
    "):\n",
    "  def forward_pass(params, embedding_lookups, labels):\n",
    "    logits = model.apply(params, embedding_lookups)\n",
    "    xentropy = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=labels\n",
    "    )\n",
    "    return jnp.mean(xentropy), logits\n",
    "\n",
    "  # Run model forward/backward pass.\n",
    "  train_step_fn = jax.value_and_grad(forward_pass, has_aux=True, allow_int=True)\n",
    "\n",
    "  (loss_val, unused_logits), grads = train_step_fn(\n",
    "      params, embedding_lookup_inputs, labels\n",
    "  )\n",
    "\n",
    "  updates, opt_state = tx.update(grads, opt_state)\n",
    "  params = embed_optimizer.apply_updates_for_sc_model(params, updates)\n",
    "\n",
    "  return params, opt_state, loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iW_z5yDsY_o"
   },
   "source": [
    "# FDO Configuration\n",
    "\n",
    "FDO (Feedback Directed Optimization) is an optional feature where we use\n",
    "batch statistics to update table limits (`max_ids_per_partition`, etc.). The FDO\n",
    "client is a generic interface and clients can either use provided\n",
    "implementations or create their own that intergrates with their infrastructure.\n",
    "When the FDO stats are used to update the limits on the `feature_specs`,\n",
    "`jax.jit` triggers a recompilation.\n",
    "\n",
    "For our flax model, updating the `FeatureSpec`s is as simple as updating the member variable on the `Model` object.\n",
    "\n",
    "In this example, we use a simple file-based implementation. Each host writes its\n",
    "own stats on `publish()` and stats from all hosts are merged during `load()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1759165868433,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "3y2BOPnWscwl",
    "outputId": "d9bc776a-2203-4af7-a1c5-ff304a2f666b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FDO storage path: /tmp/fdo_dump\n"
     ]
    }
   ],
   "source": [
    "out_path = os.path.join(FDO_DIR, 'fdo_dump')\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "print(f'FDO storage path: {out_path}')\n",
    "fdo_client = file_fdo_client.NPZFileFDOClient(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzFVKzXg4UvT"
   },
   "source": [
    "# The Training Loop\n",
    "\n",
    "The training loop is a simple Python `for`-loop. For each step, we preprocess the input batch data (on the host / CPU). In doing so, we capture the batch statistics which are used by FDO. Then we take a training step using the jitted `train_step_fn`. We finish each step by processing the training metrics and FDO stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5848,
     "status": "ok",
     "timestamp": 1759165874431,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "Yili5lon4Qeh",
    "outputId": "f4a21e5e-18d5-4efb-8a9a-594f767a247b"
   },
   "outputs": [],
   "source": [
    "step = 0\n",
    "# Track these separately for testing.\n",
    "test_loss = None\n",
    "test_step_count = 0\n",
    "for features, labels in zip(feature_batches, label_batches):\n",
    "  step += 1\n",
    "  test_step_count += 1\n",
    "  ##############################################################################\n",
    "  # SC input processing.\n",
    "  ##############################################################################\n",
    "  # These are currently global batches so each task needs to offset into\n",
    "  # the data for it's local slice.\n",
    "  labels = labels[\n",
    "      process_id * local_batch_size : (process_id + 1) * local_batch_size\n",
    "  ]\n",
    "  labels = jax.make_array_from_process_local_data(data_sharding, labels)\n",
    "\n",
    "  # Each input preprocessing processes the current process's slice of the\n",
    "  # global batch.\n",
    "  features = features[\n",
    "      process_id * local_batch_size : (process_id + 1) * local_batch_size\n",
    "  ]\n",
    "\n",
    "  model_inputs, step_stats = process_inputs(\n",
    "      feature_specs,\n",
    "      step,\n",
    "      features,\n",
    "      global_mesh,\n",
    "      data_sharding,\n",
    "      num_sc_per_device,\n",
    "  )\n",
    "  fdo_client.record(step_stats)\n",
    "\n",
    "  ##############################################################################\n",
    "  # Run the model.\n",
    "  ##############################################################################\n",
    "  # We capture the logging here so you can see when XLA compilation is\n",
    "  # triggered.\n",
    "  params, opt_state, loss_val = train_step(\n",
    "      params, model_inputs, labels, opt_state\n",
    "  )\n",
    "\n",
    "  if step % LOG_FREQUENCY == 0:\n",
    "    print(\n",
    "        f'Step {step}: loss={loss_val}, params is'\n",
    "        f' {jax.tree.map(jnp.sum, params)}'\n",
    "    )\n",
    "  test_loss = loss_val\n",
    "  if step % FDO_FREQUENCY == 0:\n",
    "    print(f'Refreshing FDO stats after step {step}')\n",
    "    fdo_client.publish()\n",
    "    loaded_stats = fdo_client.load()\n",
    "    embedding.update_preprocessing_parameters(\n",
    "        model.feature_specs, loaded_stats, num_sc_per_device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1759165874613,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "T5ZX-ehl2gGh",
    "outputId": "ed08a699-c04f-4b23-8ee4-467717f797f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss = 0.0079\n",
      "test_step_count = 100\n"
     ]
    }
   ],
   "source": [
    "# Print the loss and step count for the colab test.\n",
    "print(f'test_loss = {test_loss:.4f}')\n",
    "print(f'test_step_count = {test_step_count}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
