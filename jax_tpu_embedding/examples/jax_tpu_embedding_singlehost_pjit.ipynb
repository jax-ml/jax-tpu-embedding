{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oudeZO61GTSL"
      },
      "source": [
        "## Using jax_tpu_embedding (Single Host Pjit)\n",
        "\n",
        "This colab is to demonstrate how to use jax_tpu_embedding for training large embeddings in Jax.\n",
        "This example using embedding lookup activation results as input, and train on target.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfCN_4yAlu2C"
      },
      "outputs": [],
      "source": [
        "from absl import logging\n",
        "import functools\n",
        "from typing import Union\n",
        "\n",
        "from flax.training import common_utils\n",
        "from flax.training.train_state import TrainState\n",
        "import jax\n",
        "from jax.experimental import jax2tf\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "\n",
        "from jax_tpu_embedding import input_utils\n",
        "from jax_tpu_embedding import tpu_embedding_utils\n",
        "from jax_tpu_embedding import tpu_embedding as jte\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N__o1RDmFq0r"
      },
      "source": [
        "#### 0. Initialize TPU system for jax_tpu_embedding prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiOwIvnpaMX8"
      },
      "outputs": [],
      "source": [
        "# Note: TPUEmbedding user needs to call init_tpu_system in the beginning of program.\n",
        "tpu_embedding_utils.init_tpu_system()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-ccDHzgpdS8"
      },
      "source": [
        "#### 1. Define Example Dense Model\n",
        "\n",
        "Dense model on TPU device is a simple MLP layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuGUlugqma4j"
      },
      "outputs": [],
      "source": [
        "import flax.linen as nn\n",
        "\n",
        "Array = Union[jnp.ndarray, jnp.DeviceArray]\n",
        "Initializer = jax.nn.initializers.Initializer\n",
        "\n",
        "class MLPLayers(nn.Module):\n",
        "  \"\"\"Create mlp layers.\"\"\"\n",
        "  hidden_dim: int\n",
        "  num_hidden_layers: int\n",
        "  dropout: float\n",
        "  num_classes: int\n",
        "  kernel_init: Initializer = nn.initializers.glorot_uniform()\n",
        "  bias_init: Initializer = nn.initializers.zeros\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x: Array, is_training: bool = False) -\u003e Array:\n",
        "    for _ in range(self.num_hidden_layers):\n",
        "      x = nn.Dense(\n",
        "          features=self.hidden_dim,\n",
        "          kernel_init=self.kernel_init,\n",
        "          bias_init=self.bias_init)(\n",
        "              x)\n",
        "      x = nn.relu(x)\n",
        "\n",
        "    if is_training:\n",
        "      x = nn.Dropout(rate=self.dropout)(x, deterministic=False)\n",
        "    x = nn.Dense(features=self.num_classes, bias_init=self.bias_init)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe-7bc85HoXF"
      },
      "source": [
        "##### Define one hot targets conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQPm2f0OHnRC"
      },
      "outputs": [],
      "source": [
        "def compute_one_hot_targets(targets: Array, num_classes: int,\n",
        "                            on_value: float) -\u003e Array:\n",
        "  \"\"\"Compute one hot encoded targets.\n",
        "\n",
        "  Args:\n",
        "    targets: An array of target value.\n",
        "    num_classes: number of classes to one-hot encoding.\n",
        "    on_value: Value to fill to non-zero locations.\n",
        "  Returns:\n",
        "    An array of one-hot encoded targets.\n",
        "  \"\"\"\n",
        "  one_hot_targets = common_utils.onehot(targets, num_classes, on_value=on_value)\n",
        "  one_hot_targets = jax.tree_util.tree_map(lambda x: jnp.sum(x, axis=1),\n",
        "                                           one_hot_targets)\n",
        "  return one_hot_targets\n",
        "\n",
        "\n",
        "@jax.vmap\n",
        "def categorical_cross_entropy_loss(logits: Array, one_hot_targets: Array):\n",
        "  return -jnp.sum(one_hot_targets * nn.log_softmax(logits), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpEEgJ-58FM6"
      },
      "source": [
        "#### 2. Create dummy sample inputs\n",
        "\n",
        "We have two `watches` and `watches_targets` in dummy inputs.\n",
        "* Dense model takes embedding lookup results of `watches` and use `watches_targets` one hot target to train model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgONTxrb7-vO"
      },
      "outputs": [],
      "source": [
        "NUM_TARGET_IDS = 5\n",
        "NUM_WATCHES = 10\n",
        "\n",
        "def dummy_dataset(global_batch_size: int, vocab_size: int, num_classes: int, seed: int =123):\n",
        "  rng_state = np.random.RandomState(seed=seed)\n",
        "\n",
        "  def _create_feature():\n",
        "    watches = rng_state.randint(low=0, high=vocab_size,\n",
        "                                size=NUM_WATCHES * global_batch_size)\n",
        "    watches = tf.sparse.from_dense(watches.reshape(\n",
        "        [global_batch_size, NUM_WATCHES]))\n",
        "    targets = rng_state.randint(low=0, high=num_classes,\n",
        "                                size=NUM_TARGET_IDS * global_batch_size)\n",
        "    targets = tf.convert_to_tensor(\n",
        "        targets.reshape([global_batch_size, NUM_TARGET_IDS]))\n",
        "    return ({\n",
        "        'watches': tf.sparse.reset_shape(\n",
        "            watches, new_shape=[global_batch_size, vocab_size]),\n",
        "    }, {\n",
        "        'watches_target': tf.cast(targets, dtype=tf.float32),\n",
        "    })\n",
        "  ds = tf.data.Dataset.from_tensors(_create_feature())\n",
        "  ds = ds.repeat()\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipWYTskT7_OM"
      },
      "source": [
        "#### 3. Create Embedding Layer\n",
        "\n",
        "User needs to define feature configuration, it requires:\n",
        "* table to lookup for given feature.\n",
        "* output_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni73vLW_DHkW"
      },
      "source": [
        "##### Function to build feature configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe2NCvCz_AiW"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def build_embedding_configs(batch_size_per_device: int,\n",
        "                            embedding_dimension: int,\n",
        "                            vocab_size: int):\n",
        "  \"\"\"Create feature configurations for embedding layer.\n",
        "\n",
        "  Args:\n",
        "    batch_size_per_device: batch size of inputs to equeue.\n",
        "    embedding_dimension: dimension size of embedding table.\n",
        "    vocab_size: vocabulary size of embedding table.\n",
        "  Returns:\n",
        "    A dictionary of feature configurations.\n",
        "  \"\"\"\n",
        "  feature_configs = {\n",
        "      'watches': tf.tpu.experimental.embedding.FeatureConfig(\n",
        "          table=tf.tpu.experimental.embedding.TableConfig(\n",
        "              vocabulary_size=vocab_size,\n",
        "              dim=embedding_dimension,\n",
        "              initializer=tf.initializers.TruncatedNormal(\n",
        "                  mean=0.0, stddev=1 / math.sqrt(embedding_dimension)),\n",
        "                  combiner='mean'),\n",
        "          output_shape=[batch_size_per_device])\n",
        "  }\n",
        "  return feature_configs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDEAhHS04tuf"
      },
      "source": [
        "##### Setup flags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxziCaCc4sCz"
      },
      "outputs": [],
      "source": [
        "flags = dict(\n",
        "    global_batch_size=16,\n",
        "    embedding_dimension=64,\n",
        "    hidden_layer_dimension=32,\n",
        "    num_hidden_layers=1,\n",
        "    vocab_size=16,\n",
        "    num_classes=4,\n",
        "    learning_rate=1.0,\n",
        "    dropout=0.5,\n",
        "    num_targets=NUM_TARGET_IDS,\n",
        "    is_training=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTt4SRuSatZ5"
      },
      "source": [
        "##### Create and Initialize A TPUEmbedding Layer\n",
        "\n",
        "* Why pjit user needs TPUEmbedding SPMD?\n",
        "`pjit` is the API exposed for XLA SPMD partitioner. To align with that, pjit user needs TPUEmbedding SPMD to enable XLA sharding annotation.\n",
        "*What to set for `cores_per_replica`?\n",
        "For `pjit` model parallelism user, it is the number of tensor cores for each model replica. For `pjit` data parallelism user, it needs to be set to `jax.device_count()` or `jax.local_device_count()` for single host user only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igFzVkVEhk2H"
      },
      "outputs": [],
      "source": [
        "batch_size_per_device = flags['global_batch_size'] // jax.device_count()\n",
        "\n",
        "feature_configs = build_embedding_configs(\n",
        "      batch_size_per_device=batch_size_per_device,\n",
        "      embedding_dimension=flags['embedding_dimension'],\n",
        "      vocab_size=flags['vocab_size'],\n",
        "      )\n",
        "\n",
        "embedding_optimizer = tf.tpu.experimental.embedding.Adagrad(\n",
        "    learning_rate=flags['learning_rate'])\n",
        "tpu_embedding_layer = jte.TPUEmbedding(\n",
        "    feature_configs=feature_configs, optimizer=embedding_optimizer, \n",
        "    # Pjit user must set `cores_per_replica` to enable TPUEmbedding SPMD\n",
        "    cores_per_replica=jax.local_device_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dZrzdvv2LTR"
      },
      "outputs": [],
      "source": [
        "# Must call initialize_tpu_embedding to configure TPUEmbedding\n",
        "tpu_embedding_layer.initialize_tpu_embedding()\n",
        "\n",
        "# Call load_embedding_tables to initialize embedding tables.\n",
        "tpu_embedding_layer.load_embedding_tables()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtapktjMIQvH"
      },
      "source": [
        "#### Input pipeline\n",
        "\n",
        "We have two inputs `watches` and `watches_targets`. User may want to use data parallelism aligns TensorCores.\n",
        "\n",
        "For this example, `watches` is input data to enqueue on CPU to be processed by\n",
        "TPUEmbedding hostsoftware. `watches_targets` is input data to TensorCore for dense\n",
        "model.\n",
        "\n",
        "Therefore we split input data for host and devices by `split_and_prefetch_to_host_and_devices`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIlOnBET6n9X"
      },
      "source": [
        "##### Create Global Mesh and Partition Specs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aipcj5pvpxe9"
      },
      "outputs": [],
      "source": [
        "from jax.sharding import Mesh\n",
        "from typing import Any, Dict, Sequence, Tuple\n",
        "\n",
        "\n",
        "def create_global_mesh(mesh_shape: Tuple[int, ...],\n",
        "                       axis_names: Sequence[jax.pxla.MeshAxisName]) -\u003e Mesh:\n",
        "  size = np.prod(mesh_shape)\n",
        "  if len(jax.devices()) \u003c size:\n",
        "    raise ValueError(f'Test requires {size} global devices.')\n",
        "  devices = sorted(jax.devices(), key=lambda d: d.id)\n",
        "  mesh_devices = np.array(devices[:size]).reshape(mesh_shape)\n",
        "  global_mesh = Mesh(mesh_devices, axis_names)\n",
        "  return global_mesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivH-BU9t6Uwc"
      },
      "outputs": [],
      "source": [
        "num_devices = jax.device_count()\n",
        "mesh_axis_names = ('x',)\n",
        "\n",
        "global_mesh = create_global_mesh((num_devices,), mesh_axis_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSLS05-n7MKW"
      },
      "outputs": [],
      "source": [
        "from jax.sharding import PartitionSpec\n",
        "\n",
        "partition_spec = PartitionSpec(mesh_axis_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgTgqodp6yYz"
      },
      "source": [
        "##### Build Dummy Input Data Iterator with Data Parallelism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAGouzp76gxt"
      },
      "outputs": [],
      "source": [
        "device_input_fn = input_utils.make_pjit_array_fn(global_mesh, (partition_spec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0H1UgJRkvoq"
      },
      "outputs": [],
      "source": [
        "ds = dummy_dataset(global_batch_size=flags['global_batch_size'],\n",
        "                   vocab_size=flags['vocab_size'],\n",    
        "                   num_classes=flags['num_classes'])\n",
        "\n",
        "dummy_iter = input_utils.split_and_prefetch_to_host_and_devices(\n",
        "    iterator=iter(ds),\n",
        "    split_fn=lambda xs: {'host': xs[0], 'device': xs[1]},\n",
        "    host_input_fn=input_utils.enqueue_prefetch(\n",
        "        enqueue_fn=functools.partial(\n",
        "            tpu_embedding_layer.enqueue, is_training=flags['is_training'])),\n",
        "    device_input_fn=device_input_fn,\n",
        "    buffer_size=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8d-fcEENX32"
      },
      "source": [
        "#### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksViIauGNXJ4"
      },
      "outputs": [],
      "source": [
        "# Create TrainState\n",
        "mlp_model = MLPLayers(\n",
        "    hidden_dim=flags['hidden_layer_dimension'],\n",
        "    num_hidden_layers=flags['num_hidden_layers'],\n",
        "    dropout=flags['dropout'],\n",
        "    num_classes=flags['num_classes'])\n",
        "\n",
        "init_params = mlp_model.init(\n",
        "    jax.random.PRNGKey(123),\n",
        "    jnp.ones((batch_size_per_device, flags['embedding_dimension'])))\n",
        "tx = optax.adagrad(learning_rate=flags['learning_rate'])\n",
        "\n",
        "train_state = TrainState.create(apply_fn=mlp_model.apply, params=init_params,\n",
        "                                tx=tx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjPPnD74T1P2"
      },
      "source": [
        "##### Build Train/Eval Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUcAct2GYC64"
      },
      "outputs": [],
      "source": [
        "def build_step(embedding_layer: jte.TPUEmbedding,\n",
        "               train_state: TrainState,\n",
        "               config_flags: Dict[str, Union[int, float]],\n",
        "               is_training: bool,\n",
        "               use_pjit: bool):\n",
        "  \"\"\"Build train or eval step using tpu embedding.\"\"\"\n",
        "\n",
        "  def forward(inputs):\n",
        "    embedding_activations = inputs['embedding_actv']\n",
        "    params = inputs['params']\n",
        "    logits = train_state.apply_fn(params, embedding_activations['watches'])\n",
        "    one_hot_targets = compute_one_hot_targets(\n",
        "        inputs['watches_targets'],\n",
        "        num_classes=config_flags['num_classes'],\n",
        "        on_value=1.0 / config_flags['num_targets'])\n",
        "    loss = categorical_cross_entropy_loss(logits, one_hot_targets)\n",
        "    loss = jnp.sum(loss, axis=0) * (1.0 / config_flags['global_batch_size'])\n",
        "    return loss\n",
        "\n",
        "  def step_fn(train_state, watches_targets):\n",
        "    embedding_activation = embedding_layer.dequeue()\n",
        "    inputs = {\n",
        "        'embedding_actv': embedding_activation,\n",
        "        'params': train_state.params,\n",
        "        'watches_targets': watches_targets,\n",
        "    }\n",
        "    if is_training:\n",
        "      loss, grads = jax.value_and_grad(forward)(inputs)\n",
        "      embedding_grads, params_grads = grads['embedding_actv'], grads['params']\n",
        "      if not use_pjit:\n",
        "          params_grads = jax.lax.pmean(params_grads, axis_name='devices')\n",
        "          loss = jax.lax.pmean(loss, axis_name='devices')\n",
        "      train_state = train_state.apply_gradients(grads=params_grads)\n",
        "      embedding_layer.apply_gradients(embedding_grads)\n",
        "    else:\n",
        "      loss = forward(inputs)\n",
        "    return loss, train_state\n",
        "\n",
        "  return step_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsXdqFKESoDE"
      },
      "source": [
        "##### Run with pjit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnVlomgFcj5D"
      },
      "outputs": [],
      "source": [
        "from jax.experimental import pjit\n",
        "\n",
        "train_step_fn = build_step(\n",
        "    embedding_layer=tpu_embedding_layer,\n",
        "    train_state=train_state,\n",
        "    config_flags=flags,\n",
        "    is_training=flags['is_training'],\n",
        "    use_pjit=True)\n",
        "\n",
        "num_steps = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs_R-YLghbtD"
      },
      "source": [
        "###### Model Replicated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 457,
          "status": "ok",
          "timestamp": 1667843220192,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "nTZcuPY5TsPx",
        "outputId": "3d28aaa0-62d2-48e1-b5c5-5b01b609a19e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_step =  0 loss =  2.7713807\n",
            "train_step =  1 loss =  2.7181337\n",
            "train_step =  2 loss =  2.686849\n",
            "train_step =  3 loss =  2.6700463\n",
            "train_step =  4 loss =  2.659828\n",
            "train_step =  5 loss =  2.6517568\n",
            "train_step =  6 loss =  2.6445704\n",
            "train_step =  7 loss =  2.6370976\n",
            "train_step =  8 loss =  2.6295438\n",
            "train_step =  9 loss =  2.621694\n"
          ]
        }
      ],
      "source": [
        "with global_mesh:\n",
        "  # Replicated TrainState.\n",
        "  replicated_train_state = pjit.pjit(\n",
        "          lambda x: x,\n",
        "          in_shardings=None,\n",
        "          out_shardings=None,\n",
        "          keep_unused=True)(train_state)\n",
            "\n",
        "  for step in range(num_steps):\n",
        "    inputs = next(dummy_iter)\n",
        "    loss, replicated_train_state = pjit.pjit(\n",
        "        train_step_fn,\n",
        "        in_shardings=(None, PartitionSpec('x',)),\n",
        "        out_shardings=(None, None),\n",
        "        keep_unused=True)(\n",
            "            replicated_train_state, inputs['device']['watches_target'])\n",
        "    print('train_step = ', step, 'loss = ', loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yieVuHk2ekG8"
      },
      "source": [
        "###### Model Parallelism\n",
        "\n",
        "* Prepare axis resources for train state sharding along devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gObNZP6y6WNb"
      },
      "outputs": [],
      "source": [
        "import optax\n",
        "from flax.core import scope as flax_scope\n",
        "\n",
        "params_resources = flax_scope.FrozenDict({\n",
        "    'params': {\n",
        "        'Dense_0': {\n",
        "            'kernel': PartitionSpec('x', None),\n",
        "            'bias': PartitionSpec('x',),\n",
        "        },\n",
        "        'Dense_1': {\n",
        "            'kernel': PartitionSpec('x', None),\n",
        "            'bias': PartitionSpec('x',),\n",
        "        },\n",
        "    },\n",
        "})\n",
        "\n",
        "sharded_axis_resources = TrainState(\n",
        "    step=PartitionSpec(), apply_fn=train_state.apply_fn,  \n",
        "    params=params_resources, \n",
        "    tx=train_state.tx, \n",
        "    opt_state=(\n",
        "        optax.ScaleByRssState(\n",
        "            sum_of_squares=params_resources), optax.EmptyState()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 497,
          "status": "ok",
          "timestamp": 1667844493511,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "lWTRx_T18b0K",
        "outputId": "a0db543f-97aa-49fd-9035-eef308f2ce37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_step =  0 loss =  2.7485476\n",
            "train_step =  1 loss =  2.6802473\n",
            "train_step =  2 loss =  2.652364\n",
            "train_step =  3 loss =  2.6370351\n",
            "train_step =  4 loss =  2.6250236\n",
            "train_step =  5 loss =  2.6136596\n",
            "train_step =  6 loss =  2.6020913\n",
            "train_step =  7 loss =  2.5901182\n",
            "train_step =  8 loss =  2.5761333\n",
            "train_step =  9 loss =  2.559519\n"
          ]
        }
      ],
      "source": [
        "with global_mesh:\n",
        "  # Replicated TrainState.\n",
        "  sharded_train_state = pjit.pjit(\n",
        "          lambda x: x,\n",
        "          in_shardings=None,\n",
        "          out_shardings=sharded_axis_resources,\n",
        "          keep_unused=True)(train_state)\n",
        "\n",
        "  for step in range(num_steps):\n",
        "    inputs = next(dummy_iter)\n",
        "    loss, sharded_train_state = pjit.pjit(\n",
        "        train_step_fn,\n",
        "        in_shardings=(sharded_axis_resources, PartitionSpec('x',)),\n",
        "        out_shardings=(None, sharded_axis_resources), \n",
        "        keep_unused=True)(\n",
        "            sharded_train_state, inputs['device']['watches_target'])\n",
        "    print('train_step = ', step, 'loss = ', loss)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/brain/experimental/jax_tpu_embedding/examples/colab:colab_binary",
        "kind": "private"
      },
      "name": "jax_tpu_embedding_singlehost_pjit.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
